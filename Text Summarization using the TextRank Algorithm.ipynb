{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "807ad369",
   "metadata": {},
   "source": [
    "# Text Summarization using the TextRank Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67210473",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af323401",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1c6d9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b9caea",
   "metadata": {},
   "source": [
    "### Loading CSV created after Stemming on Sentences\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2df33851",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_s = pd.read_csv('df_sc_s.csv', index_col = [0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14766580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentences</th>\n",
       "      <th>length</th>\n",
       "      <th>word_count</th>\n",
       "      <th>length_change</th>\n",
       "      <th>wordcount_change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dubai build code 2021 edit content part part b...</td>\n",
       "      <td>791</td>\n",
       "      <td>134</td>\n",
       "      <td>183</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>content dbc base follow input 1) exist regul t...</td>\n",
       "      <td>251</td>\n",
       "      <td>35</td>\n",
       "      <td>76</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dbc arrang theme integr relev element build de...</td>\n",
       "      <td>166</td>\n",
       "      <td>26</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2 dubai build code part gener a.2 definit note...</td>\n",
       "      <td>131</td>\n",
       "      <td>22</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a.2.1 term addit increas building’ gross area ...</td>\n",
       "      <td>80</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Sentences  length  word_count  \\\n",
       "0  dubai build code 2021 edit content part part b...     791         134   \n",
       "1  content dbc base follow input 1) exist regul t...     251          35   \n",
       "2  dbc arrang theme integr relev element build de...     166          26   \n",
       "3  2 dubai build code part gener a.2 definit note...     131          22   \n",
       "4  a.2.1 term addit increas building’ gross area ...      80          12   \n",
       "\n",
       "   length_change  wordcount_change  \n",
       "0            183                 0  \n",
       "1             76                 0  \n",
       "2             42                 0  \n",
       "3             17                 0  \n",
       "4              6                 0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_s.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a9a16f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for only First time when downloading glove embedding model.\n",
    "\n",
    "\n",
    "\n",
    "# import wget\n",
    "# import unzip\n",
    "\n",
    "# wget.download('http://nlp.stanford.edu/data/glove.6B.zip')\n",
    "\n",
    "# import zipfile\n",
    "\n",
    "# with zipfile.ZipFile('glove.6B.zip', 'r') as zip_ref:\n",
    "#     zip_ref.extractall('glove')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e487ef3",
   "metadata": {},
   "source": [
    "### Vector Representation of Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "954bbb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extract word vectors\n",
    "# word_embeddings = {}\n",
    "# f = open('./glove/glove.6B.100d.txt', encoding='utf-8')\n",
    "# for line in f:\n",
    "#     values = line.split()\n",
    "#     word = values[0]\n",
    "#     coefs = np.asarray(values[1:], dtype='float32')\n",
    "#     word_embeddings[word] = coefs\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "da867de8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4b110591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence_vectors = []\n",
    "# for i in df_s['Sentences']:\n",
    "#   if len(i) != 0:\n",
    "#     v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
    "#   else:\n",
    "#     v = np.zeros((100,))\n",
    "#   sentence_vectors.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "093e44a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s_length = len(df_s['Sentences'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543d2d6c",
   "metadata": {},
   "source": [
    "### Similarity Matrix Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2385ee89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # similarity matrix\n",
    "# sim_mat = np.zeros([s_length, s_length])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552d1a0f",
   "metadata": {},
   "source": [
    "We will use Cosine Similarity to compute the similarity between a pair of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51aa23b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# from scipy.spatial.distance import cosine\n",
    "# import numpy as np\n",
    "# import scipy.sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dd20031e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # initialize the matrix with cosine similarity scores.\n",
    "\n",
    "# for i in range(s_length):\n",
    "#   for j in range(s_length):\n",
    "#     if i != j:\n",
    "#       sim_mat[i][j] = cosine(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cd57d59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(s_length):\n",
    "#   for j in range(s_length):\n",
    "#     if i != j:\n",
    "#       sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "70635755",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'scipy.sparse' has no attribute 'coo_array'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [63]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnetworkx\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnx\u001b[39;00m\n\u001b[0;32m      3\u001b[0m nx_graph \u001b[38;5;241m=\u001b[39m nx\u001b[38;5;241m.\u001b[39mfrom_numpy_array(sim_mat)\n\u001b[1;32m----> 4\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpagerank\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnx_graph\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\networkx\\algorithms\\link_analysis\\pagerank_alg.py:108\u001b[0m, in \u001b[0;36mpagerank\u001b[1;34m(G, alpha, personalization, max_iter, tol, nstart, weight, dangling)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpagerank\u001b[39m(\n\u001b[0;32m     10\u001b[0m     G,\n\u001b[0;32m     11\u001b[0m     alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.85\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m     dangling\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     18\u001b[0m ):\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;124;03m\"\"\"Returns the PageRank of the nodes in the graph.\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \n\u001b[0;32m     21\u001b[0m \u001b[38;5;124;03m    PageRank computes a ranking of the nodes in the graph G based on\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    106\u001b[0m \n\u001b[0;32m    107\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 108\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpagerank_scipy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[43m        \u001b[49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpersonalization\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdangling\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\networkx\\algorithms\\link_analysis\\pagerank_alg.py:469\u001b[0m, in \u001b[0;36mpagerank_scipy\u001b[1;34m(G, alpha, personalization, max_iter, tol, nstart, weight, dangling)\u001b[0m\n\u001b[0;32m    466\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {}\n\u001b[0;32m    468\u001b[0m nodelist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(G)\n\u001b[1;32m--> 469\u001b[0m A \u001b[38;5;241m=\u001b[39m \u001b[43mnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_scipy_sparse_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnodelist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnodelist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    470\u001b[0m S \u001b[38;5;241m=\u001b[39m A\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    471\u001b[0m S[S \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m S[S \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\networkx\\convert_matrix.py:921\u001b[0m, in \u001b[0;36mto_scipy_sparse_array\u001b[1;34m(G, nodelist, dtype, weight, format)\u001b[0m\n\u001b[0;32m    919\u001b[0m         r \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m diag_index\n\u001b[0;32m    920\u001b[0m         c \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m diag_index\n\u001b[1;32m--> 921\u001b[0m     A \u001b[38;5;241m=\u001b[39m \u001b[43msp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoo_array\u001b[49m((d, (r, c)), shape\u001b[38;5;241m=\u001b[39m(nlen, nlen), dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    922\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    923\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m A\u001b[38;5;241m.\u001b[39masformat(\u001b[38;5;28mformat\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'scipy.sparse' has no attribute 'coo_array'"
     ]
    }
   ],
   "source": [
    "# import networkx as nx\n",
    "\n",
    "# nx_graph = nx.from_numpy_array(sim_mat)\n",
    "# scores = nx.pagerank(nx_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd7b831",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d731be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
